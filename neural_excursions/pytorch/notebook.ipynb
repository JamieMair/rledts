{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598370110676",
   "display_name": "Python 3.7.7 64-bit ('pytorch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Torch Version {torch.__version__}\")\n",
    "print(f\"Torch Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "\n",
    "    def __init__(self, T: int):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.T = T\n",
    "        self.hidden1 = nn.Linear(2, 64)\n",
    "        self.hidden2 = nn.Linear(64, 32)\n",
    "        self.softmax_layer = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert x to be between -1 and 1\n",
    "        x = x / self.T - 0.5\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.sigmoid(self.softmax_layer(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "\n",
    "    def __init__(self, T: int):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.T = T\n",
    "        self.hidden1 = nn.Linear(2, 64)\n",
    "        self.hidden2 = nn.Linear(64, 32)\n",
    "        self.value_layer = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert x to be between -1 and 1\n",
    "        x = x / self.T - 0.5\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.value_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=100 # The length of the excursion\n",
    "negative_reward = -50\n",
    "lose_reward_gradient = -500\n",
    "\n",
    "policy_learning_rate = 0.0001\n",
    "value_learning_rate = 0.0001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"results/models/\"\n",
    "import os\n",
    "if not os.path.exists(\"results/\"):\n",
    "    os.mkdir(\"results/\")\n",
    "if not os.path.exists(model_dir) or not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "with open(os.path.join(model_dir, 'params.txt'), 'w') as writer:\n",
    "    writer.write(f\"T: {T}\\n\")\n",
    "    writer.write(f\"negative_reward: {negative_reward}\\n\")\n",
    "    writer.write(f\"lose_reward_gradient: {lose_reward_gradient}\\n\")\n",
    "    writer.write(f\"policy_learning_rate: {policy_learning_rate}\\n\")\n",
    "    writer.write(f\"value_learning_rate: {value_learning_rate}\\n\")\n",
    "    writer.write(f\"batch_size: {batch_size}\\n\")\n",
    "def save_models(epoch: int, policy_net, value_net):\n",
    "    save_dir = os.path.join(model_dir, f\"epoch_{epoch}\")\n",
    "    os.mkdir(save_dir)\n",
    "    policy_path = os.path.join(save_dir, \"policy.pt\")\n",
    "    value_path = os.path.join(save_dir, \"value.pt\") \n",
    "    torch.save(policy_net, policy_path)\n",
    "    torch.save(value_net, value_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the neural networks for the policy and the value function\n",
    "policy_net = PolicyNet(T) # Policy outputs the probability of the up action\n",
    "value_net = ValueNet(T) # Value outputs a single number representing the value of the state (x,t)\n",
    "# LOAD PREVIOUS WORK\n",
    "# Define the optimisers for the gradient descent\n",
    "policy_optim = optim.Adam(policy_net.parameters(), lr=policy_learning_rate)\n",
    "value_optim = optim.Adam(value_net.parameters(), lr=value_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "total_mean_returns = []\n",
    "mean_successes = []\n",
    "mean_entropies = []\n",
    "mean_hits = []\n",
    "for epoch in range(epochs):\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}\")\n",
    "\n",
    "    policy_optim.zero_grad()\n",
    "    value_optim.zero_grad()\n",
    "    total_reward_mean = 0\n",
    "    total_mean_entropy = 0\n",
    "    state = torch.zeros(batch_size, 2)\n",
    "    success = (torch.ones(batch_size, 1)).type(torch.BoolTensor)\n",
    "    for t in range(T):\n",
    "\n",
    "        prob_policy = policy_net(state)\n",
    "        value_state = value_net(state)\n",
    "        random_sample = torch.rand(batch_size, 1)\n",
    "\n",
    "        action = (random_sample<prob_policy).type(torch.FloatTensor)\n",
    "        action.requires_grad_(False)\n",
    "\n",
    "        prob_action = action * prob_policy + (1.0-action)*(1.0-prob_policy)\n",
    "\n",
    "        delta_state = torch.cat((action*2.0-1.0, torch.ones(batch_size, 1)), 1)\n",
    "        # action True = move up, action False = move down\n",
    "\n",
    "        next_state = state + delta_state\n",
    "        \n",
    "        below_zero_bool = next_state[:, 0] < 0\n",
    "        success = torch.logical_and(torch.logical_not(below_zero_bool), success)\n",
    "        below_zero = (below_zero_bool).type(torch.FloatTensor).reshape((-1, 1))\n",
    "        \n",
    "        if (t<T-1):\n",
    "            base_reward = below_zero * negative_reward\n",
    "            next_value = torch.zeros(batch_size, 1)\n",
    "        else:\n",
    "            base_reward = below_zero * negative_reward + torch.abs(next_state[:, 0].reshape((-1, 1))) * lose_reward_gradient\n",
    "            has_hit = next_state[:, 0] == 0.0\n",
    "            success = torch.logical_and(has_hit, success)\n",
    "            next_value = value_net(next_state)\n",
    "\n",
    "        entropy_term = -torch.log(prob_action).detach()\n",
    "        total_mean_entropy += torch.mean(entropy_term)\n",
    "        total_reward = (base_reward + entropy_term).detach()\n",
    "        total_reward_mean += torch.mean(total_reward)\n",
    "        # discount = 1.0\n",
    "        temp_diff = total_reward + next_value.detach() - value_state\n",
    "        value_loss = torch.mean(temp_diff * temp_diff)\n",
    "        value_loss.backward()\n",
    "        policy_loss = -torch.mean(temp_diff.detach() * torch.log(prob_action))\n",
    "        policy_loss.backward()\n",
    "        state = next_state\n",
    "    \n",
    "    policy_optim.step()\n",
    "    value_optim.step()\n",
    "    total_mean_returns.append(total_reward_mean.numpy())\n",
    "    mean_successes.append((torch.mean(success.type(torch.FloatTensor))).numpy())\n",
    "    mean_hits.append((torch.mean(has_hit.type(torch.FloatTensor))).numpy())\n",
    "    mean_entropies.append(total_mean_entropy.numpy())\n",
    "    if (epoch % 10 == 0):\n",
    "        save_models(epoch, policy_net, value_net)\n",
    "\n",
    "mean_return = np.mean(total_mean_returns)\n",
    "with open(os.path.join(model_dir, 'run_data.csv'), 'w') as writer:\n",
    "    writer.write(\"epoch,mean_entropy,mean_return,mean_success,mean_hit_rate\\n\")\n",
    "    for i in range(len(mean_entropies)):\n",
    "        writer.write(f\"{i},{mean_entropies[i]},{total_mean_returns[i]},{mean_successes[i]},{mean_hits[i]}\\n\")\n",
    "print(f\"Peak Mean Return {np.max(total_mean_returns)}\")\n",
    "print(f\"Peak Mean Entropy {np.max(mean_entropies)}\")\n",
    "plt.figure()\n",
    "plt.title('Mean Return')\n",
    "plt.plot([0, epochs-1], [0, 0], '-b')\n",
    "plt.plot(range(epochs), total_mean_returns, '-r')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.title('Mean Success')\n",
    "plt.plot(range(epochs), mean_successes, '-r')\n",
    "plt.ylim([0, 1])\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.title('Mean Hits')\n",
    "plt.plot(range(epochs), mean_hits, '-r')\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1024\n",
    "loaded_policy = torch.load('results/models/epoch_990/policy.pt')\n",
    "total_reward_mean = 0\n",
    "state = torch.zeros(num_samples, 2)\n",
    "success = (torch.ones(num_samples, 1)).type(torch.BoolTensor)\n",
    "\n",
    "states = []\n",
    "for t in range(T):\n",
    "\n",
    "    prob_policy = loaded_policy(state)\n",
    "    random_sample = torch.rand(num_samples, 1)\n",
    "\n",
    "    action = (random_sample<prob_policy).type(torch.FloatTensor)\n",
    "    action.requires_grad_(False)\n",
    "\n",
    "    prob_action = action * prob_policy + (1.0-action)*(1.0-prob_policy)\n",
    "\n",
    "    delta_state = torch.cat((action*2.0-1.0, torch.ones(num_samples, 1)), 1)\n",
    "    # action True = move up, action False = move down\n",
    "    states.append(state[:, 0].numpy())\n",
    "    state = state + delta_state\n",
    "states.append(state[:, 0].numpy())\n",
    "\n",
    "states = np.array(states)\n",
    "plt.figure()\n",
    "plt.title(\"Sampled Paths\")\n",
    "plt.xlabel(\"Time, t\")\n",
    "plt.ylabel(\"Position, x\")\n",
    "for i in range(num_samples):\n",
    "    plt.plot(range(T+1), states[:,i], '-k', alpha=0.005)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 990\n",
    "folder = f'results/models/epoch_{epoch}'\n",
    "loaded_policy = torch.load(f'results/models/epoch_{epoch}/policy.pt')\n",
    "with open(os.path.join(folder, 'policy.csv'), 'w') as writer:\n",
    "    writer.write(\"x,t,policy_up,policy_down\\n\")\n",
    "\n",
    "    for t in range(T):\n",
    "        for x in range(-t, t+1, 2):\n",
    "            state = torch.Tensor([x, t]).reshape((-1, 2))\n",
    "            policy_val = np.asscalar(loaded_policy(state).detach().numpy())\n",
    "            writer.write(f\"{x},{t},{policy_val},{1.0-policy_val}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}